{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1 \n",
    "\"\"\" \n",
    "Web scraping is an automatic method to obtain large amounts of data from websites. \n",
    "Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. \n",
    "There are many different ways to perform web scraping to obtain data from websites. \n",
    "These include using online services, particular API’s or even creating your code for web scraping from scratch. \n",
    "Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. \n",
    "This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Lead Generation for Marketing\n",
    "\"\"\" \n",
    "A web scraping software can be used to generate leads for marketing. \n",
    "Email and Phone lists for cold outreach can be built by scraping the data from relevant websites\n",
    "\"\"\"\n",
    "\n",
    "# 2. Price Comparison & Competition Monitoring\n",
    "\"\"\"\n",
    "Companies catering products or services need to have comprehensive data of competitor products and services which appear in the market every day\n",
    "\"\"\"\n",
    "\n",
    "# 3. Data Analysis\n",
    "\"\"\"\n",
    "We might want to collect and analyze data related to a specific category from multiple websites. \n",
    "The category might be real estate, automobiles, electronic gadgets, industrial equipment, business contacts, marketing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2 \n",
    "\n",
    "# 1 . HTML PARSING\n",
    "\"\"\"\n",
    "HTML parsing is done with JavaScript and targets linear or nested HTML pages. \n",
    "It is a fast and robust method that is used for text extraction, screen scraping, and resource extraction among others.\n",
    "\"\"\"\n",
    "\n",
    "# 2. DOM PARSING\n",
    "\"\"\"\n",
    "DOM is short for Document Object Model and it defines the style structure and content of XML files. \n",
    "Scrapers make use of DOM parsers to get an in-depth view of a web page’s structure. \n",
    "They can also use a DOM parser to get nodes containing information and then use a tool like XPath to scrape web pages. \n",
    "Internet Explorer or Firefox browsers can be embedded to extract the entire web page or just parts of it.\n",
    "\"\"\"\n",
    "\n",
    "# 3 .GOOGLE SHEETS\n",
    "\"\"\"\n",
    "Google sheets are a web scraping tool that is quite popular among web scrapers. \n",
    "From within sheets, a scraper can make use of IMPORT XML (,) function to scrape as much data as is needed from websites. \n",
    "This method is only useful when specific data or patterns are required from a website. \n",
    "You can also use this command to check if your website is secure from scraping.\n",
    "\"\"\"\n",
    "\n",
    "# 4. TEXT PATTERN MATCHING\n",
    "\"\"\"\n",
    "This is a matching technique that involves the use of the UNIX grep command and is used with popular programming languages like Perl or Python.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3 \n",
    "\n",
    "\"\"\" \n",
    "Beautiful Soup is a Python library that is commonly used for web scraping purposes. \n",
    "It is designed to make parsing HTML and XML documents easy and intuitive.\n",
    "\n",
    "Web scraping involves extracting data from websites, and Beautiful Soup can be used to extract data from HTML or XML documents. \n",
    "It provides a simple and intuitive way to navigate and search the document structure, allowing developers to easily extract the data they need.\n",
    "\n",
    "Some of the key features of Beautiful Soup include:\n",
    "\n",
    "1. it can navigate and search an HTML/XML document using CSS selectors or regular expressions\n",
    "2. parse poorly formatted HTML/XML documents\n",
    "3. The ability to handle different encodings and character sets\n",
    "4. convert parsed documents to Unicode\n",
    "5. ability to work with different parsers, including the built-in Python parser and external parsers like lxml and html5lib\n",
    "\n",
    "Beautiful Soup is used for a variety of purposes, including:\n",
    "\n",
    "Data mining and analysis\n",
    "Research and academic purposes\n",
    "Automated testing and quality assurance\n",
    "Web development and content management\n",
    "Overall, Beautiful Soup is a powerful tool for web scraping and data extraction, and its ease of use and versatility have made it a popular choice among developers.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "\"\"\" \n",
    "Flask is a good choice for web scraping projects because:\n",
    "\n",
    "It is lightweight and easy to use, making it ideal for small to medium-sized projects\n",
    "It is flexible and customizable, allowing developers to create custom routes, views, and templates\n",
    "It has a built-in web server, which makes it easy to test and deploy web applications\n",
    "It has a large and active community of developers, which means there is a wealth of documentation, tutorials, and support available\n",
    "\n",
    "Flask is a good choice for a web scraping project because it provides a simple and flexible way to create a web interface for the web scraper also flask is very light and fast, allowing users to easily specify the data they want to extract from a web page.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5 \n",
    "\n",
    "\n",
    "\n",
    "# Two main AWS services used are\n",
    "# 1. Code Pipeline\n",
    "# 2. Elastic Beanstalk \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "CODE PIPELINE\n",
    "\n",
    "AWS CodePipeline is a continuous delivery service that enables you to model, visualize, and automate the steps required to release your software. \n",
    "We can integrate it with our GitHub Repo.\n",
    "With AWS CodePipeline, you model the full release process for building your code, deploying to pre-production environments, testing your application and releasing it to production. \n",
    "AWS CodePipeline then builds, tests, and deploys your application according to the defined workflow every time there is a code change. \n",
    "\n",
    "\n",
    "ELASTIC BEANSTALK\n",
    "Elastic Beanstalk is a service for deploying and scaling web applications and services. \n",
    "It connects with code pipline as well to manage the code\n",
    "after uploading code the Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
